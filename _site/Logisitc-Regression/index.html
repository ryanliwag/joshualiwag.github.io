<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>Logistic Regression - Personal websites for projects</title>

  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="Personal websites for projects" property="og:site_name">
  
    <meta content="Logistic Regression" property="og:title">
  
  
    <meta content="article" property="og:type">
  
  
    <meta content="Just some machine learning basics" property="og:description">
  
  
    <meta content="http://localhost:4000/Logisitc-Regression/" property="og:url">
  
  
    <meta content="2018-02-23T15:32:20+08:00" property="article:published_time">
    <meta content="http://localhost:4000/about/" property="article:author">
  
  
    <meta content="/img/main_pic.jpg" property="og:image">
  
  
    
  
  
    
  

  
    <meta name="twitter:card" content="Just some machine learning basics">
  
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@ryanorchous">
  
    <meta name="twitter:title" content="Logistic Regression">
  
  
    <meta name="twitter:url" content="http://localhost:4000/Logisitc-Regression/">
  
  
    <meta name="twitter:description" content="Just some machine learning basics">
  
  
    <meta name="twitter:image:src" content="http://localhost:4000/assets/img/main_pic.jpg">
  

	<meta name="description" content="Just some machine learning basics">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/assets/img/favicon/favicon.ico" type="image/x-icon">
	<link rel="apple-touch-icon" href="/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicon/apple-touch-icon-114x114.png">
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/assets/css/main.css">
</head>

<body>

  <div class="wrapper">
    <aside class="sidebar">
  <header>
    <div class="about">
      <div class="cover-author-image">
        <a href="/"><img src="/assets/img/main_pic.jpg" alt="Ryan Liwag"></a>
      </div>
      <div class="author-name">Ryan Liwag</div>
      <p>Fresh Grad Weeb looking for employment. I hope to work on anything related to AI and ML. "A lesson without pain is meaningless. For you cannot gain something without sacrificing something else in return." - Edward Elric</p>
    </div>
  </header> <!-- End Header -->
  <footer>
    <section class="contact">
      <h3 class="contact-title">Contact me</h3>
      <ul>
        
          <li><a href="https://twitter.com/ryanorchous" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
        
        
          <li><a href="https://facebook.com/ryan.joshua.75" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a></li>
        
        
          <li class="github"><a href="http://github.com/ryanliwag" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
          <li class="linkedin"><a href="https://in.linkedin.com/in/ryan-joshua-liwag-4a6124138/" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        
        
          <li class="email"><a href="mailto:rjhontomin@gmail.com"><i class="fa fa-envelope-o"></i></a></li>
        
      </ul>
    </section> <!-- End Section Contact -->
    <div class="copyright">
      <p>2018 &copy; Ryan Liwag</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->
<div class="content-box clearfix">
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>
<article class="article-page">
  <div class="page-content">
    
    
    <div class="wrap-content">
      <header class="header-page">
        <h1 class="page-title">Logistic Regression</h1>
        <div class="page-date"><span>2018, Feb 23&nbsp;&nbsp;&nbsp;&nbsp;</span></div>
      </header>
      <p>It has always bothered me to use scikit-learn library to some extent, not because it’s bad but because I can’t seem to understand the underlying principle of the theories behind each machine learning procedure. This post, I will try to concisely explain how logistic regression operates at least in a mathematical level which is low enough so that if ever I need to, I can reproduce the code in numpy.</p>

<h2 id="logistic-regression">Logistic regression</h2>

<h3 id="overview">Overview</h3>

<p>Logistic regression is comparable to how a neural network operates, in fact, it could even be identified as a 1 layer neural network. Logistic regression can be used for either binary or multiclass classification (one for all method or softmax). So despite being likened to a neural network, it is still a linear model, due to its calculation always depending upon the sum of the inputs and parameters.</p>

<h3 id="logistic-regression-and-linear-regression">Logistic Regression and Linear Regression</h3>

<p>To start off, the main difference between a linear regression and logistic regression, is the implementation of the sigmoid function at its linear formula which is  Y = wX + b , this same formula is for calculating the slope of a line and this is what linear regression does, which is fitting a line that minimizes error through the data. But this gives a continuous output, on the other hand, logistic regression gives a binary output. This binary output is caused by a sigmoid function applied to the linear formula that gives the probabilities of the prediction. The sigmoid function which is  <script type="math/tex">\sigma(z) =  \frac{1}{1 + e^{-(z)}}</script>   has the characteristic of an s-shaped curve.</p>

<p>Sigmoid will be applied as such <script type="math/tex">\sigma(w^T x + b) =  \frac{1}{1 + e^{-(w^T x + b)}}</script> . This function is the basis of logistic regression, the output of this function is used to calculate the cost function for optimizing and once wiegths are optimized this is the model used for predicting.</p>

<p>Python Code</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

<span class="c">#Forward propagation process</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="cost-function-and-optimization">Cost Function and Optimization</h3>

<p>Its the main objective of any machine learning model is to optimize and this is can be done through minimizing the loss function through a process of gradient descent. The cost function can in simpler terms be stated as the summary of the loss function.</p>

<p>Loss Function:  <script type="math/tex">\mathcal{L}(\hat{y},y) = -(y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)}))</script></p>

<p>Cost Function:   <script type="math/tex">\mathcal{J}(w,b) = -\frac{1}{m}\sum_{i=1}^{m}\mathcal{L}(\hat{y},y)</script></p>

<p>The process of decreasing the cost function is most commonly known as gradient descent. There is a bit of calculus involved with getting the gradients but that’s about it.</p>

<p>The gradient of the loss with respect to w: <script type="math/tex">\frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T</script></p>

<p>The gradient of the loss with respect to b: <script type="math/tex">\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})</script></p>

<p>When these grads are calculated they are then multiplied by the set learning rate and is subtracted from the previous weight values. This update process repeats for as many iterations that I set.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="c"># compute activation</span>
<span class="n">cost</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">Y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">A</span><span class="p">))</span> <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">A</span><span class="p">))))</span>   <span class="c"># compute cost</span>

<span class="c"># BACKWARD PROPAGATION</span>
<span class="n">grad_w</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">grad_b</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">A</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span>

<span class="c"># Updating the w and b values</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="p">(</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="p">(</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gra_b</span><span class="p">)</span>
</code></pre></div></div>

<p>so the steps required to complete the process is</p>

<p>1- Initialize the weights for (w &amp; b), they can be random values or could be intialized at zero.</p>

<p>2- Calculate the gradient, change in the cost function when their values are changes by a small amount from the original. This improves the values of (w &amp; b) in the direction in which the cost function is minimized.</p>

<p>3- Adjust the weights using the gradients to reach optimial value.</p>

<p>4- Use the weights to predict and get a new cost.</p>

<p>5- Steps 2-3 will repeat for number of set iterations, and will keep adjusting the weigths, ideally untill no error can no longer be significantly reduced.</p>

<p>Full Python code for <a href="https://github.com/ryanliwag/ryanliwag.github.io/blob/master/notebooks/logistic-regression.ipynb">logistic regression binary classification</a> implemented on the breast cancer dataset</p>

      <div class="page-footer">
        <div class="page-share">
          <a href="https://twitter.com/intent/tweet?text=Logistic Regression&url=http://localhost:4000/Logisitc-Regression/" title="Share on Twitter" rel="nofollow" target="_blank">Twitter</a>
          <a href="https://facebook.com/sharer.php?u=http://localhost:4000/Logisitc-Regression/" title="Share on Facebook" rel="nofollow" target="_blank">Facebook</a>
          <a href="https://plus.google.com/share?url=http://localhost:4000/Logisitc-Regression/" title="Share on Google+" rel="nofollow" target="_blank">Google+</a>
        </div>
        <div class="page-tag">
          
        </div>
      </div>
    </div> <!-- End Wrap Content -->
  </div> <!-- End Page Content -->
</article> <!-- End Article Page -->

</div>

  </div>

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', '', 'auto');
  ga('send', 'pageview');
</script> <!-- End Analytics -->

</body>
</html>
